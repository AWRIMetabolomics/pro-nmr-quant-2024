{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Workbook\n",
    "\n",
    "* Given a learnt gradient, predict value of conc in samples. \n",
    "* Run in a batch of size mayb 20 so not all 1000 wines are loaded into memory all at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh.plotting import figure, show, output_file, output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import integrate\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/dteng/Documents/bin/nmr_utils/\")\n",
    "from nmr_targeted_utils import *\n",
    "from nmr_fitting_funcs import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results to be stored in ./results/wt-lr-batch-20240412_2255\n"
     ]
    }
   ],
   "source": [
    "# params\n",
    "template_path = \"/Users/dteng/Documents/zdata/nmr/nmr_std_data/indiv_std_lproline/lproline_ph3.csv\"\n",
    "bs_grad_path = \"/Users/dteng/Documents/nmr_targeted/mlgrad-weighted-lr-5apr2024/results/bootstrap_results_5apr2024.csv\"\n",
    "matching_path = \"/Users/dteng/Documents/zdata/nmr/nmr_std_data/lr_matching_coords/lproline_ph3_matching_regions.csv\"\n",
    "#template_path = \"/Users/dteng/Documents/bin/nmr_constants/cal_data_pro/pro_stds/pro_std_03_r1.csv\"\n",
    "path_samples = \"/Users/dteng/Documents/zdata/nmr/J202208B_pro_survey/J202208B_n1214_csvs\"\n",
    "\n",
    "# diff mcoords for neat-pro-std or pro_std_03\n",
    "if \"pro_std_03\" in template_path:\n",
    "    multiplets_ls = [[1.9,2.15], [2.304, 2.408],[3.25, 3.5],[4.1, 4.2]]\n",
    "if \"lproline_ph3\" in template_path:\n",
    "    multiplets_ls = [[1.9,2.15], [2.295, 2.403], [3.25, 3.5],[4.1, 4.2]]\n",
    "std_matching_method = \"bounded-lmse\"\n",
    "\n",
    "#signal_free_coords = [-1, 10] # signal free region is outside of these coords\n",
    "\n",
    "normxcorr_th = 0.0 # set to this number to filter out multiplets which aren't at least normxcorr_th, i.e. poor fits\n",
    "ref_pk_window = [-0.02, 0.02]\n",
    "ref_pk_tolerance_window = [0,0]\n",
    "search_region_padding_size = 0.02\n",
    "\n",
    "suffix = template_path.split(\"/\")[-1].replace(\".csv\", \"\")\n",
    "fn_out_plot = f\"mlgrad_{suffix}.html\"\n",
    "fn_out_df = f\"mlgrad_{suffix}.csv\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "folder_name = f\"./results/wt-lr-batch-{timestamp}\"\n",
    "print(f\"Results to be stored in {folder_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data\n",
    "\n",
    "Data isn't loaded all at one shot, because that's just inviting out-of-memory error for very large datasets, but loaded (in the next section) and processed in batches of size `batch_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 5000 bootstrapped gradients\n",
      "1123 samples loaded into 12 batches of size:\n",
      "1. 100\n",
      "2. 100\n",
      "3. 100\n",
      "4. 100\n",
      "5. 100\n",
      "6. 100\n",
      "7. 100\n",
      "8. 100\n",
      "9. 100\n",
      "10. 100\n",
      "11. 100\n",
      "12. 23\n",
      "Results to be stored in ./results/wt-lr-batch-20240412_2255 (not yet created)\n"
     ]
    }
   ],
   "source": [
    "# ========== load data ==========\n",
    "# load STD template(s)\n",
    "template_df = pd.read_csv(template_path)\n",
    "template_df = adjust_to_ref_peak(template_df, ref_pk_window, ref_pk_tolerance_window)\n",
    "\n",
    "# load bootstrap gradients\n",
    "d_bs_grad = pd.read_csv(bs_grad_path)\n",
    "print(f\"Loaded {len(d_bs_grad)} bootstrapped gradients\")\n",
    "\n",
    "matching_regions_ls = [\n",
    "    [2.305, 2.306],\n",
    "    [2.31, 2.316],\n",
    "    [2.321, 2.3225],\n",
    "    [2.331, 2.333],\n",
    "    [2.342, 2.3445],\n",
    "    [2.347, 2.349],\n",
    "    [2.3585, 2.3605],\n",
    "    [2.365, 2.3675],\n",
    "    [2.3755, 2.3765],\n",
    "    [2.381, 2.39]]\n",
    "\n",
    "# load data filenames to batch up\n",
    "fn_ls = []\n",
    "for fn in os.listdir(path_samples):\n",
    "    if \".csv\" in fn:\n",
    "        fn_ls.append(fn)\n",
    "fn_ls = sorted(fn_ls)\n",
    "batch_size = 100\n",
    "batches_ls = [fn_ls[i:i+batch_size] for i in range(0, len(fn_ls), batch_size)]\n",
    "\n",
    "counter = 1\n",
    "print(f\"{len(fn_ls)} samples loaded into {len(batches_ls)} batches of size:\")\n",
    "for batch in batches_ls:\n",
    "    print(f\"{counter}. {len(batch)}\")\n",
    "    counter += 1\n",
    "\n",
    "# display output folder, but don't create it yet\n",
    "print(f\"Results to be stored in {folder_name} (not yet created)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created folder ./results/wt-lr-batch-20240412_2255\n",
      "Processing batch 1 of 12\n",
      "WARNING: shifting blues instead of reds in get_blue_m1_dict().\n",
      "normxcorr for AF43588 too low (0.6987968375896702), returning -1 instead\n",
      "normxcorr for AF60975 too low (0.48381327436568217), returning -1 instead\n",
      "Processing batch 2 of 12\n",
      "WARNING: shifting blues instead of reds in get_blue_m1_dict().\n",
      "Processing batch 3 of 12\n",
      "WARNING: shifting blues instead of reds in get_blue_m1_dict().\n",
      "normxcorr for AF62827_Reacquire too low (0.6245908926491924), returning -1 instead\n",
      "normxcorr for AF63412_R1 too low (0.5223211498603312), returning -1 instead\n",
      "normxcorr for AF63412_R2 too low (0.5219120503667406), returning -1 instead\n",
      "normxcorr for AF63412_Reacquire too low (0.15006926370952675), returning -1 instead\n",
      "normxcorr for AF63414 too low (0.5839849987734096), returning -1 instead\n",
      "normxcorr for AF63414_Reacquire too low (0.5927613194597555), returning -1 instead\n",
      "Processing batch 4 of 12\n",
      "WARNING: shifting blues instead of reds in get_blue_m1_dict().\n",
      "Processing batch 5 of 12\n",
      "WARNING: shifting blues instead of reds in get_blue_m1_dict().\n",
      "Processing batch 6 of 12\n",
      "WARNING: shifting blues instead of reds in get_blue_m1_dict().\n",
      "normxcorr for AF66896_Reacquire too low (0.6242655062012795), returning -1 instead\n",
      "normxcorr for AF70917_Reacquire too low (0.44492726287927303), returning -1 instead\n",
      "normxcorr for AF73789 too low (0.14080958118088044), returning -1 instead\n",
      "normxcorr for AF73790_Reacquire too low (0.46399990733349683), returning -1 instead\n",
      "normxcorr for AF73791 too low (0.5558140970323896), returning -1 instead\n",
      "normxcorr for AF73797 too low (0.040634220105553294), returning -1 instead\n",
      "normxcorr for AF73798 too low (0.17345868543366352), returning -1 instead\n",
      "normxcorr for AF73800 too low (0.4488137122922831), returning -1 instead\n",
      "normxcorr for AF73920 too low (0.28320702207284737), returning -1 instead\n",
      "normxcorr for AF73921 too low (0.47452400407502277), returning -1 instead\n",
      "normxcorr for AF73935 too low (-0.12351557056821982), returning -1 instead\n",
      "normxcorr for AF73937 too low (0.43480354010349775), returning -1 instead\n",
      "normxcorr for AF73938 too low (0.15776614878934084), returning -1 instead\n",
      "normxcorr for AF73939 too low (-0.1114279567193061), returning -1 instead\n",
      "normxcorr for AF74835 too low (0.3479857844086487), returning -1 instead\n",
      "normxcorr for AF74836 too low (0.43295192801795135), returning -1 instead\n",
      "normxcorr for AF74837 too low (0.445220601259193), returning -1 instead\n",
      "normxcorr for AF74880 too low (-0.12990198824702134), returning -1 instead\n",
      "normxcorr for AF74885 too low (0.5116277126201042), returning -1 instead\n",
      "normxcorr for AF74887 too low (0.4271269569793904), returning -1 instead\n",
      "normxcorr for AF74910 too low (0.5023364177309724), returning -1 instead\n",
      "normxcorr for AF74912_QC1 too low (0.5368953203739383), returning -1 instead\n",
      "Processing batch 7 of 12\n",
      "WARNING: shifting blues instead of reds in get_blue_m1_dict().\n",
      "normxcorr for AF74912_R1 too low (0.5016167870196862), returning -1 instead\n",
      "normxcorr for AF74912_R2 too low (0.5576672241985452), returning -1 instead\n",
      "normxcorr for AF75597 too low (0.27673265031694966), returning -1 instead\n",
      "normxcorr for AF75628 too low (0.4187753433343233), returning -1 instead\n",
      "normxcorr for AF75670 too low (0.47324581050140285), returning -1 instead\n",
      "normxcorr for AF75671_QC1 too low (0.5797748883947065), returning -1 instead\n",
      "normxcorr for AF75671_QC2 too low (0.4845093767474695), returning -1 instead\n",
      "normxcorr for AF75671_R1 too low (0.06324357092571824), returning -1 instead\n",
      "normxcorr for AF75671_R2 too low (0.5629699534372137), returning -1 instead\n",
      "normxcorr for AF75672 too low (0.27146521380939115), returning -1 instead\n",
      "normxcorr for AR1 too low (0.3267152797635133), returning -1 instead\n",
      "normxcorr for AR2 too low (0.4234411669013468), returning -1 instead\n",
      "normxcorr for AR3 too low (0.489337475630395), returning -1 instead\n",
      "normxcorr for AR5 too low (-0.09714605061301185), returning -1 instead\n",
      "normxcorr for AR6 too low (0.4496655419190208), returning -1 instead\n",
      "normxcorr for AW1 too low (0.45963679352132475), returning -1 instead\n",
      "normxcorr for AW10 too low (0.5786305913688887), returning -1 instead\n",
      "normxcorr for AW11 too low (0.06953312285485308), returning -1 instead\n",
      "normxcorr for AW12 too low (0.3256950122213425), returning -1 instead\n",
      "normxcorr for AW13 too low (0.6062774687609535), returning -1 instead\n",
      "normxcorr for AW14 too low (0.6496435173465187), returning -1 instead\n",
      "normxcorr for AW14_Reacquire too low (0.5599570713526406), returning -1 instead\n",
      "normxcorr for AW15 too low (0.5743390740591036), returning -1 instead\n",
      "normxcorr for AW17 too low (0.4684803978151623), returning -1 instead\n",
      "normxcorr for AW2 too low (0.6039659568377201), returning -1 instead\n",
      "normxcorr for AW2_Reacquire too low (0.716246777634133), returning -1 instead\n",
      "normxcorr for AW3 too low (0.3007225436794759), returning -1 instead\n",
      "normxcorr for AW4 too low (0.4374368760108357), returning -1 instead\n",
      "normxcorr for AW5 too low (0.26587751438717905), returning -1 instead\n",
      "normxcorr for AW8 too low (0.0425819019750085), returning -1 instead\n",
      "normxcorr for AW9 too low (0.2183825399597215), returning -1 instead\n",
      "Processing batch 8 of 12\n",
      "WARNING: shifting blues instead of reds in get_blue_m1_dict().\n",
      "normxcorr for AWAC18 too low (0.6720805898414194), returning -1 instead\n",
      "normxcorr for AWAC21 too low (0.6305680557276974), returning -1 instead\n",
      "normxcorr for AWAC213 too low (0.7040251528783783), returning -1 instead\n",
      "normxcorr for AWAC213_Reacquire too low (0.7407340544340111), returning -1 instead\n",
      "normxcorr for AWAC220 too low (0.7016248027521271), returning -1 instead\n",
      "Processing batch 9 of 12\n",
      "WARNING: shifting blues instead of reds in get_blue_m1_dict().\n",
      "normxcorr for AWAC23 too low (0.6512571793109716), returning -1 instead\n",
      "normxcorr for AWAC29 too low (0.739493725814289), returning -1 instead\n",
      "normxcorr for AWAC65 too low (0.6712017219834181), returning -1 instead\n",
      "normxcorr for AWAC84 too low (0.7240515652706814), returning -1 instead\n",
      "normxcorr for AWAC85 too low (0.5723158441123795), returning -1 instead\n",
      "normxcorr for AWAC86 too low (0.6247484103035359), returning -1 instead\n",
      "normxcorr for AWAC87 too low (0.4940944337562982), returning -1 instead\n",
      "normxcorr for AWAC89 too low (0.6363099233970098), returning -1 instead\n",
      "normxcorr for AWAC90 too low (0.6371015166749969), returning -1 instead\n",
      "normxcorr for AWAC91 too low (0.7279006438998353), returning -1 instead\n",
      "Processing batch 10 of 12\n",
      "WARNING: shifting blues instead of reds in get_blue_m1_dict().\n",
      "normxcorr for CS_28_QC1_Reacquire too low (0.718347417186289), returning -1 instead\n",
      "normxcorr for CS_28_QC2_Reacquire too low (0.7166766012551978), returning -1 instead\n",
      "normxcorr for CS_28_R1_Reacquire too low (0.6856751581788015), returning -1 instead\n",
      "normxcorr for CS_28_R2_Reacquire too low (0.7244243856706333), returning -1 instead\n",
      "normxcorr for FW1 too low (0.021590329635778206), returning -1 instead\n",
      "normxcorr for FW2 too low (0.5929015686415633), returning -1 instead\n",
      "normxcorr for FW3 too low (0.44688063175108933), returning -1 instead\n",
      "normxcorr for FW4 too low (0.35252268236053974), returning -1 instead\n",
      "normxcorr for June_02_Reacquire too low (0.44018462416891063), returning -1 instead\n",
      "Processing batch 11 of 12\n",
      "WARNING: shifting blues instead of reds in get_blue_m1_dict().\n",
      "normxcorr for QP1 too low (0.4933211181718149), returning -1 instead\n",
      "normxcorr for QP2 too low (0.5115793403913472), returning -1 instead\n",
      "normxcorr for QP3 too low (0.41397765460707486), returning -1 instead\n",
      "normxcorr for QP4 too low (-0.138890415359818), returning -1 instead\n",
      "normxcorr for QP5 too low (0.37651776318007024), returning -1 instead\n",
      "normxcorr for QP6 too low (0.485022459629459), returning -1 instead\n",
      "normxcorr for QP7 too low (0.18250963682074203), returning -1 instead\n",
      "normxcorr for ST174 too low (0.7025507595795701), returning -1 instead\n",
      "Processing batch 12 of 12\n",
      "WARNING: shifting blues instead of reds in get_blue_m1_dict().\n",
      "normxcorr for ST580 too low (0.5393416053496737), returning -1 instead\n",
      "normxcorr for WYNNS2_QC1 too low (0.45190171847181326), returning -1 instead\n",
      "normxcorr for WYNNS2_QC2 too low (-0.07750248394808446), returning -1 instead\n",
      "normxcorr for WYNNS2_R1 too low (0.022213848184162173), returning -1 instead\n",
      "normxcorr for WYNNS2_R2 too low (0.3026540866686846), returning -1 instead\n",
      "Done in 426.25s\n"
     ]
    }
   ],
   "source": [
    "# make folder\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "print(f\"Created folder {folder_name}\")\n",
    "\n",
    "# create to store slopes and intercepts\n",
    "d_lr_results_ls = []\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "# get reds\n",
    "red_dt = template_df.copy()\n",
    "red_dt = red_dt.loc[(red_dt[\"ppm\"]>min(multiplets_ls[1])) & (red_dt[\"ppm\"]<max(multiplets_ls[1]))]\n",
    "\n",
    "blue_m1_dict_ls = []\n",
    "d_rho_ls = []\n",
    "batch_num = 1\n",
    "for batch in batches_ls:\n",
    "    print(f\"Processing batch {batch_num} of {len(batches_ls)}\")\n",
    "    # ===== load batch data =====\n",
    "    df_dict = {}\n",
    "    for fn in batch:\n",
    "        dt = pd.read_csv(os.path.join(path_samples, fn))\n",
    "        df_dict[fn.replace(\".csv\", \"\")] = adjust_to_ref_peak(\n",
    "            dt, \n",
    "            ref_coords=ref_pk_window\n",
    "        )\n",
    "\n",
    "    # ===== run 1d_std_search =====\n",
    "    results_dict = {}\n",
    "    for k in sorted(list(df_dict.keys())):\n",
    "        target_df = df_dict[k]\n",
    "        results_dict[k] = do_1d_std_search(\n",
    "            query_df=template_df,\n",
    "            target_df=target_df,\n",
    "            multiplets_ls=multiplets_ls,\n",
    "            search_region_padding_size=search_region_padding_size\n",
    "        )\n",
    "\n",
    "    # get blues\n",
    "    blue_m1_dict = get_blue_m1_dict(results_dict, \n",
    "                                    df_dict,\n",
    "                                    mcoords=multiplets_ls[1]\n",
    "                                   )\n",
    "    blue_m1_dict_ls.append(blue_m1_dict)\n",
    "\n",
    "    # ===== get corr_series_dict =====\n",
    "    # get corr_series for each k, stored in corr_series_dict\n",
    "    corr_series_dict = {}\n",
    "    for k in sorted(list(results_dict.keys())):\n",
    "        dt = get_correlation_series(red_dt, \n",
    "                                    blue_m1_dict[k].copy(),\n",
    "                                    min_corr=0, \n",
    "                                    min_corr_replacement_value=0,\n",
    "                                    window_size_nrows=64,\n",
    "                                    exponent=8\n",
    "                                   )\n",
    "        corr_series_dict[k] = dt\n",
    "\n",
    "    # ===== run LR matching =====\n",
    "    df_conc = get_df_conc_lrmatching(\n",
    "        results_dict=results_dict, \n",
    "        template_df=template_df.copy(), \n",
    "        df_dict=df_dict, \n",
    "        mcoords=multiplets_ls[1],\n",
    "        matching_coords_ls=matching_regions_ls,\n",
    "        corr_series_dict=corr_series_dict\n",
    "    )\n",
    "    \n",
    "    d_lr_results_ls.append(df_conc)\n",
    "    \n",
    "    # ===== get conc_pred =====\n",
    "    # get AUCs multiplied by all bootstrapped gradients in a matrix\n",
    "    # get conc_pred\n",
    "    auc_m = np.matmul(df_conc[\"auc\"].values.reshape(-1, 1), \n",
    "                      d_bs_grad.values.reshape(1, -1))\n",
    "\n",
    "    # get ave +/- 95% CI\n",
    "    c = []\n",
    "    for i in range(len(auc_m)):\n",
    "        ave = np.average(auc_m[i])\n",
    "        std = np.std(auc_m[i], ddof=1)\n",
    "        ci_lower = np.percentile(auc_m[i], 2.5)\n",
    "        ci_upper = np.percentile(auc_m[i], 97.5)\n",
    "        c.append([df_conc[\"sample_name\"].values[i], ave, std, ci_lower, ci_upper])\n",
    "\n",
    "    d_concpred = pd.DataFrame(data=c, \n",
    "                              columns=[\"sample_name\", \n",
    "                                       \"conc_pred_ave\", \n",
    "                                       \"conc_pred_sd\", \n",
    "                                       \"95_ci_lower\", \n",
    "                                       \"95_ci_upper\"]\n",
    "                             )\n",
    "    \n",
    "    # ===== get normxcorrs =====\n",
    "    c = []\n",
    "    for k in sorted(list(results_dict.keys())):\n",
    "        max_rho = results_dict[k][\"multiplet_1\"][\"max_rho\"][0]\n",
    "        c.append([k, max_rho])\n",
    "    d_max_rho = pd.DataFrame(data=c, columns=[\"sample_name\", \"normxcorr\"])\n",
    "    \n",
    "    d_concpred = pd.merge(d_concpred, d_max_rho, on=\"sample_name\")\n",
    "    d_rho_ls.append(d_concpred[[\"sample_name\", \"normxcorr\"]])\n",
    "    \n",
    "    # ===== plot match results =====\n",
    "    fig, ax = plt.subplots(nrows=len(results_dict), # top row for LR results\n",
    "                           ncols=1, \n",
    "                           figsize=(5, len(results_dict)*3)\n",
    "                          )\n",
    "\n",
    "    red_dt = template_df.copy()\n",
    "    red_dt = red_dt.loc[(red_dt[\"ppm\"]>min(multiplets_ls[1])) & (red_dt[\"ppm\"]<max(multiplets_ls[1]))]\n",
    "\n",
    "    i = 0\n",
    "    for k in sorted(list(results_dict.keys())):\n",
    "        # plot fit\n",
    "        normxcorr = results_dict[k][\"multiplet_1\"][\"max_rho\"][0]\n",
    "        ax[i].plot(blue_m1_dict[k].ppm.values, \n",
    "                   blue_m1_dict[k].intensity.values, c=\"steelblue\")\n",
    "\n",
    "        m = df_conc.loc[df_conc[\"sample_name\"]==k][\"slope\"].values[0]\n",
    "        c = df_conc.loc[df_conc[\"sample_name\"]==k][\"intercept\"].values[0]\n",
    "        ax[i].plot(red_dt.ppm.values, \n",
    "                   (red_dt.intensity.values*m)+c, \n",
    "                   c=\"indianred\")\n",
    "\n",
    "        ax[i].set_title(f\"{i+1}. {k}\\nnormxcorr={round(normxcorr, 4)}\")\n",
    "        \n",
    "        ax[i].plot(corr_series_dict[k][\"ppm\"].values, \n",
    "                   corr_series_dict[k][\"corr_series\"].values,\n",
    "                   ls=\"--\",\n",
    "                   lw=0.5,\n",
    "                   c=\"k\"\n",
    "                  )\n",
    "\n",
    "        # set bg colour\n",
    "        bg_colour = \"#FE9FA5\" # red\n",
    "        if normxcorr >= 0.85 and normxcorr < 0.9:\n",
    "            bg_colour = \"#FFB863\" # orange\n",
    "        if normxcorr >= 0.9 and normxcorr < 0.95:\n",
    "            bg_colour = \"#FFF263\" # yellow     \n",
    "        if normxcorr >= 0.95 and normxcorr < 0.99:\n",
    "            bg_colour = \"#96FEBF\" # green\n",
    "        elif normxcorr >= 0.99:\n",
    "            bg_colour = \"#8CDCFF\" # light blue\n",
    "        ax[i].set_facecolor(bg_colour)\n",
    "        ax[i].set_alpha(0.7)\n",
    "        plt.setp(ax[i].get_xticklabels(), fontsize=20)\n",
    "        plt.setp(ax[i].get_yticklabels(), fontsize=20)\n",
    "\n",
    "        # draw matching regions as gray bars\n",
    "        rect_height = ax[i].get_ylim()[1]\n",
    "        for row in matching_regions_ls:\n",
    "            # Create a rectangle patch\n",
    "            rect = patches.Rectangle((min(row), 0), \n",
    "                                     max(row) - min(row), \n",
    "                                     rect_height, \n",
    "                                     edgecolor=None,\n",
    "                                     facecolor='grey', \n",
    "                                     alpha=0.25)\n",
    "    \n",
    "            # Add the rectangle patch to the plot\n",
    "            ax[i].add_patch(rect)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    # final bits\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0)\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "\n",
    "    i = StringIO()\n",
    "    fig.savefig(i, format=\"svg\")\n",
    "    output_svg = i.getvalue().strip().split(\"\\n\")\n",
    "    svg_ls = resize_svg(output_svg, resize_coeff=0.5)\n",
    "    \n",
    "    # ===== prep html report =====\n",
    "    # not written as a function because of the sheer number of inputs required\n",
    "    html_contents = [\"<html><head></head><body>\"]\n",
    "    html_contents.append(\"<ul>\")\n",
    "    html_contents.append(f\"<li>Report generated: {datetime.today().strftime('%d %b %y, %-I:%M%p')}</li>\")\n",
    "    html_contents.append(f\"<li>num_samples = {len(results_dict)}</li>\")\n",
    "    html_contents.append(f\"<li>template = {template_path.split('/')[-1]}</li>\")\n",
    "    html_contents.append(f\"<li>normxcorr threshold = {normxcorr_th}</li>\")\n",
    "    html_contents.append(f\"<li>ref peak window = {ref_pk_window}</li>\")\n",
    "    html_contents.append(f\"<li>ref peak tolerance window = {ref_pk_tolerance_window}</li>\")\n",
    "    html_contents.append(f\"<li>search region padding size (ppm) = {search_region_padding_size}</li>\")\n",
    "    html_contents.append(\"</ul>\")\n",
    "\n",
    "    html_contents.append(\"<hr>\")\n",
    "    for line in output_svg:\n",
    "        html_contents.append(line)\n",
    "    html_contents.append(\"</body></html>\")\n",
    "\n",
    "    # ===== write out =====\n",
    "    # write out html\n",
    "    fn_out_plot = f\"batch{batch_num}_viz.html\"\n",
    "    with open(f\"./{folder_name}/{fn_out_plot}\", \"w\") as f:\n",
    "        for line in html_contents:\n",
    "            f.write(line)\n",
    "\n",
    "    # write out csvs\n",
    "    dz = pd.merge(d_concpred, df_conc, on=\"sample_name\")\n",
    "    dz.to_csv(os.path.join(folder_name, f\"batch{batch_num}_concpred.csv\"), index=False)\n",
    "    \n",
    "    batch_num += 1\n",
    "\n",
    "print(\"Done in %.2fs\" % (time.perf_counter() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get csv of results\n",
    "dz_rho = pd.concat(d_rho_ls, axis=0).reset_index(drop=True)\n",
    "dz_lr_results = pd.concat(d_lr_results_ls, axis=0).reset_index(drop=True)\n",
    "\n",
    "# merge\n",
    "dz = pd.merge(dz_rho, dz_lr_results, on=\"sample_name\")\n",
    "print(len(dz))\n",
    "\n",
    "# write out\n",
    "dz.to_csv(f\"{folder_name}/conc_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! open ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
