{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Production Workbook\n",
    "\n",
    "* Given a learnt gradient, predict value of conc in samples. \n",
    "* Run in a batch of size mayb 20 so not all 1000 wines are loaded into memory all at once.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from bokeh.plotting import figure, show, output_file, output_notebook\n",
    "from bokeh.models import ColumnDataSource, HoverTool\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', None)\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from io import StringIO\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy import integrate\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"/Users/dteng/Documents/bin/nmr_utils/\")\n",
    "from nmr_targeted_utils import *\n",
    "from nmr_fitting_funcs import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "# params\n",
    "template_path = \"/Users/dteng/Documents/zdata/nmr/nmr_std_data/indiv_std_lproline/lproline_ph3.csv\"\n",
    "bs_grad_path = \"/Users/dteng/Documents/nmr_targeted/mlgrad-weighted-lr-5apr2024/results/bootstrap_results_5apr2024.csv\"\n",
    "matching_path = \"/Users/dteng/Documents/zdata/nmr/nmr_std_data/lr_matching_coords/lproline_ph3_matching_regions.csv\"\n",
    "#template_path = \"/Users/dteng/Documents/bin/nmr_constants/cal_data_pro/pro_stds/pro_std_03_r1.csv\"\n",
    "path_samples = \"/Users/dteng/Documents/zdata/nmr/J202208B_pro_survey/extra_fid_data_csvs\"\n",
    "\n",
    "# diff mcoords for neat-pro-std or pro_std_03\n",
    "if \"pro_std_03\" in template_path:\n",
    "    multiplets_ls = [[1.9,2.15], [2.304, 2.408],[3.25, 3.5],[4.1, 4.2]]\n",
    "if \"lproline_ph3\" in template_path:\n",
    "    multiplets_ls = [[1.9,2.15], [2.295, 2.403], [3.25, 3.5],[4.1, 4.2]]\n",
    "std_matching_method = \"bounded-lmse\"\n",
    "\n",
    "#signal_free_coords = [-1, 10] # signal free region is outside of these coords\n",
    "\n",
    "normxcorr_th = 0.0 # set to this number to filter out multiplets which aren't at least normxcorr_th, i.e. poor fits\n",
    "ref_pk_window = [-0.02, 0.02]\n",
    "ref_pk_tolerance_window = [0,0]\n",
    "search_region_padding_size = 0.02\n",
    "\n",
    "suffix = template_path.split(\"/\")[-1].replace(\".csv\", \"\")\n",
    "fn_out_plot = f\"mlgrad_{suffix}.html\"\n",
    "fn_out_df = f\"mlgrad_{suffix}.csv\"\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M\")\n",
    "folder_name = f\"./results/wt-lr-batch-{timestamp}\"\n",
    "print(f\"Results to be stored in {folder_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ========== load data ==========\n",
    "# load STD template(s)\n",
    "template_df = pd.read_csv(template_path)\n",
    "template_df = adjust_to_ref_peak(template_df, ref_pk_window, ref_pk_tolerance_window)\n",
    "\n",
    "# load bootstrap gradients\n",
    "d_bs_grad = pd.read_csv(bs_grad_path)\n",
    "print(f\"Loaded {len(d_bs_grad)} bootstrapped gradients\")\n",
    "\n",
    "matching_regions_ls = [\n",
    "    [2.305, 2.306],\n",
    "    [2.31, 2.316],\n",
    "    [2.321, 2.3225],\n",
    "    [2.331, 2.333],\n",
    "    [2.342, 2.3445],\n",
    "    [2.347, 2.349],\n",
    "    [2.3585, 2.3605],\n",
    "    [2.365, 2.3675],\n",
    "    [2.3755, 2.3765],\n",
    "    [2.381, 2.39]]\n",
    "\n",
    "# load data filenames to batch up\n",
    "fn_ls = []\n",
    "for fn in os.listdir(path_samples):\n",
    "    if \".csv\" in fn:\n",
    "        fn_ls.append(fn)\n",
    "fn_ls = sorted(fn_ls)\n",
    "batch_size = 100\n",
    "batches_ls = [fn_ls[i:i+batch_size] for i in range(0, len(fn_ls), batch_size)]\n",
    "\n",
    "counter = 1\n",
    "print(f\"Loaded into {len(batches_ls)} batches of size:\")\n",
    "for batch in batches_ls:\n",
    "    print(f\"{counter}. {len(batch)}\")\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make folder\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "print(f\"Created folder {folder_name}\")\n",
    "\n",
    "# create to store slopes and intercepts\n",
    "d_lr_results_ls = []\n",
    "t0 = time.perf_counter()\n",
    "\n",
    "# get reds\n",
    "red_dt = template_df.copy()\n",
    "red_dt = red_dt.loc[(red_dt[\"ppm\"]>min(multiplets_ls[1])) & (red_dt[\"ppm\"]<max(multiplets_ls[1]))]\n",
    "\n",
    "blue_m1_dict_ls = []\n",
    "d_rho_ls = []\n",
    "batch_num = 1\n",
    "for batch in batches_ls:\n",
    "    print(f\"Processing batch {batch_num} of {len(batches_ls)}\")\n",
    "    # ===== load batch data =====\n",
    "    df_dict = {}\n",
    "    for fn in batch:\n",
    "        dt = pd.read_csv(os.path.join(path_samples, fn))\n",
    "        df_dict[fn.replace(\".csv\", \"\")] = adjust_to_ref_peak(\n",
    "            dt, \n",
    "            ref_coords=ref_pk_window\n",
    "        )\n",
    "\n",
    "    # ===== run 1d_std_search =====\n",
    "    results_dict = {}\n",
    "    for k in sorted(list(df_dict.keys())):\n",
    "        target_df = df_dict[k]\n",
    "        results_dict[k] = do_1d_std_search(\n",
    "            query_df=template_df,\n",
    "            target_df=target_df,\n",
    "            multiplets_ls=multiplets_ls,\n",
    "            search_region_padding_size=search_region_padding_size\n",
    "        )\n",
    "\n",
    "    # get blues\n",
    "    blue_m1_dict = get_blue_m1_dict(results_dict, \n",
    "                                    df_dict,\n",
    "                                    mcoords=multiplets_ls[1]\n",
    "                                   )\n",
    "    blue_m1_dict_ls.append(blue_m1_dict)\n",
    "\n",
    "    # ===== get corr_series_dict =====\n",
    "    # get corr_series for each k, stored in corr_series_dict\n",
    "    corr_series_dict = {}\n",
    "    for k in sorted(list(results_dict.keys())):\n",
    "        dt = get_correlation_series(red_dt, \n",
    "                                    blue_m1_dict[k].copy(),\n",
    "                                    min_corr=0, \n",
    "                                    min_corr_replacement_value=0,\n",
    "                                    window_size_nrows=64,\n",
    "                                    exponent=8\n",
    "                                   )\n",
    "        corr_series_dict[k] = dt\n",
    "\n",
    "    # ===== run LR matching =====\n",
    "    df_conc = get_df_conc_lrmatching(\n",
    "        results_dict=results_dict, \n",
    "        template_df=template_df.copy(), \n",
    "        df_dict=df_dict, \n",
    "        mcoords=multiplets_ls[1],\n",
    "        matching_coords_ls=matching_regions_ls,\n",
    "        corr_series_dict=corr_series_dict\n",
    "    )\n",
    "    \n",
    "    d_lr_results_ls.append(df_conc)\n",
    "    \n",
    "    # ===== get conc_pred =====\n",
    "    # get AUCs multiplied by all bootstrapped gradients in a matrix\n",
    "    # get conc_pred\n",
    "    auc_m = np.matmul(df_conc[\"auc\"].values.reshape(-1, 1), \n",
    "                      d_bs_grad.values.reshape(1, -1))\n",
    "\n",
    "    # get ave +/- 95% CI\n",
    "    c = []\n",
    "    for i in range(len(auc_m)):\n",
    "        ave = np.average(auc_m[i])\n",
    "        std = np.std(auc_m[i], ddof=1)\n",
    "        ci_lower = np.percentile(auc_m[i], 2.5)\n",
    "        ci_upper = np.percentile(auc_m[i], 97.5)\n",
    "        c.append([df_conc[\"sample_name\"].values[i], ave, std, ci_lower, ci_upper])\n",
    "\n",
    "    d_concpred = pd.DataFrame(data=c, \n",
    "                              columns=[\"sample_name\", \n",
    "                                       \"conc_pred_ave\", \n",
    "                                       \"conc_pred_sd\", \n",
    "                                       \"95_ci_lower\", \n",
    "                                       \"95_ci_upper\"]\n",
    "                             )\n",
    "    \n",
    "    # ===== get normxcorrs =====\n",
    "    c = []\n",
    "    for k in sorted(list(results_dict.keys())):\n",
    "        max_rho = results_dict[k][\"multiplet_1\"][\"max_rho\"][0]\n",
    "        c.append([k, max_rho])\n",
    "    d_max_rho = pd.DataFrame(data=c, columns=[\"sample_name\", \"normxcorr\"])\n",
    "    \n",
    "    d_concpred = pd.merge(d_concpred, d_max_rho, on=\"sample_name\")\n",
    "    d_rho_ls.append(d_concpred[[\"sample_name\", \"normxcorr\"]])\n",
    "    \n",
    "    # ===== plot match results =====\n",
    "    fig, ax = plt.subplots(nrows=len(results_dict), # top row for LR results\n",
    "                           ncols=1, \n",
    "                           figsize=(7, len(results_dict)*4)\n",
    "                          )\n",
    "\n",
    "    red_dt = template_df.copy()\n",
    "    red_dt = red_dt.loc[(red_dt[\"ppm\"]>min(multiplets_ls[1])) & (red_dt[\"ppm\"]<max(multiplets_ls[1]))]\n",
    "\n",
    "    i = 0\n",
    "    for k in sorted(list(results_dict.keys())):\n",
    "        # plot fit\n",
    "        normxcorr = results_dict[k][\"multiplet_1\"][\"max_rho\"][0]\n",
    "        ax[i].plot(blue_m1_dict[k].ppm.values, \n",
    "                   blue_m1_dict[k].intensity.values, c=\"steelblue\")\n",
    "\n",
    "        m = df_conc.loc[df_conc[\"sample_name\"]==k][\"slope\"].values[0]\n",
    "        c = df_conc.loc[df_conc[\"sample_name\"]==k][\"intercept\"].values[0]\n",
    "        ax[i].plot(red_dt.ppm.values, \n",
    "                   (red_dt.intensity.values*m)+c, \n",
    "                   c=\"indianred\")\n",
    "\n",
    "        ax[i].set_title(f\"{i+1}. {k}\\nnormxcorr={round(normxcorr, 4)}\")\n",
    "        \n",
    "        ax[i].plot(corr_series_dict[k][\"ppm\"].values, \n",
    "                   corr_series_dict[k][\"corr_series\"].values,\n",
    "                   ls=\"--\",\n",
    "                   lw=0.5,\n",
    "                   c=\"k\"\n",
    "                  )\n",
    "\n",
    "        # set bg colour\n",
    "        bg_colour = \"#FE9FA5\" # red\n",
    "        if normxcorr >= 0.85 and normxcorr < 0.9:\n",
    "            bg_colour = \"#FFB863\" # orange\n",
    "        if normxcorr >= 0.9 and normxcorr < 0.95:\n",
    "            bg_colour = \"#FFF263\" # yellow     \n",
    "        if normxcorr >= 0.95 and normxcorr < 0.99:\n",
    "            bg_colour = \"#96FEBF\" # green\n",
    "        elif normxcorr >= 0.99:\n",
    "            bg_colour = \"#8CDCFF\" # light blue\n",
    "        ax[i].set_facecolor(bg_colour)\n",
    "        ax[i].set_alpha(0.7)\n",
    "        plt.setp(ax[i].get_xticklabels(), fontsize=20)\n",
    "        plt.setp(ax[i].get_yticklabels(), fontsize=20)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    plt.subplots_adjust(hspace=0.2, wspace=0)\n",
    "    plt.tight_layout()\n",
    "    plt.close()\n",
    "\n",
    "    i = StringIO()\n",
    "    fig.savefig(i, format=\"svg\")\n",
    "    output_svg = i.getvalue().strip().split(\"\\n\")\n",
    "    svg_ls = resize_svg(output_svg, resize_coeff=0.5)\n",
    "    \n",
    "    # ===== prep html report =====\n",
    "    # not written as a function because of the sheer number of inputs required\n",
    "    html_contents = [\"<html><head></head><body>\"]\n",
    "    html_contents.append(\"<ul>\")\n",
    "    html_contents.append(f\"<li>Report generated: {datetime.today().strftime('%d %b %y, %-I:%M%p')}</li>\")\n",
    "    html_contents.append(f\"<li>num_samples = {len(results_dict)}</li>\")\n",
    "    html_contents.append(f\"<li>template = {template_path.split('/')[-1]}</li>\")\n",
    "    html_contents.append(f\"<li>normxcorr threshold = {normxcorr_th}</li>\")\n",
    "    html_contents.append(f\"<li>ref peak window = {ref_pk_window}</li>\")\n",
    "    html_contents.append(f\"<li>ref peak tolerance window = {ref_pk_tolerance_window}</li>\")\n",
    "    html_contents.append(f\"<li>search region padding size (ppm) = {search_region_padding_size}</li>\")\n",
    "    html_contents.append(\"</ul>\")\n",
    "\n",
    "    html_contents.append(\"<hr>\")\n",
    "    for line in output_svg:\n",
    "        html_contents.append(line)\n",
    "    html_contents.append(\"</body></html>\")\n",
    "\n",
    "    # ===== write out everything =====\n",
    "    fn_out_plot = f\"batch{batch_num}_viz.html\"\n",
    "    with open(f\"./{folder_name}/{fn_out_plot}\", \"w\") as f:\n",
    "        for line in html_contents:\n",
    "            f.write(line)\n",
    "\n",
    "    d_concpred.to_csv(os.path.join(folder_name, f\"batch{batch_num}_concpred.csv\"), index=False)\n",
    "    \n",
    "    batch_num += 1\n",
    "\n",
    "print(\"Done in %.2fs\" % (time.perf_counter() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dz_rho = pd.concat(d_rho_ls, axis=0).reset_index(drop=True)\n",
    "dz_lr_results = pd.concat(d_lr_results_ls, axis=0).reset_index(drop=True)\n",
    "\n",
    "# merge\n",
    "dz = pd.merge(dz_rho, dz_lr_results, on=\"sample_name\")\n",
    "print(len(dz))\n",
    "\n",
    "# write out\n",
    "dz.to_csv(f\"{folder_name}/conc_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
